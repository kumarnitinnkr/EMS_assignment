{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f89542f",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch — Letter Recognition (A, B, C)\n",
    "\n",
    "**Module 11 Project**: Implement a simple feedforward neural network using only NumPy to classify binary 5×6 images of letters **A**, **B**, and **C**. The network uses a single hidden layer with **sigmoid** activation and is trained via custom backpropagation (mean squared error loss). The notebook includes visualization of letter images, training loss & accuracy plots, and final predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9250c9",
   "metadata": {},
   "source": [
    "## 1. Setup and synthetic dataset\n",
    "We'll define binary pixel patterns for A, B and C as 5x6 images (height=5, width=6), flatten them to 30-element vectors, and create augmented samples by adding small random flips so the network has enough data to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe8540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 5x6 binary patterns for letters A, B, C.\n",
    "# Use lists of 5 rows x 6 columns (0/1). We'll flatten to 30-length vectors.\n",
    "\n",
    "A = np.array([\n",
    "    [0,1,1,1,1,0],\n",
    "    [0,1,0,0,0,1],\n",
    "    [0,1,1,1,1,1],\n",
    "    [0,1,0,0,0,1],\n",
    "    [0,1,0,0,0,1]\n",
    "], dtype=float)\n",
    "\n",
    "B = np.array([\n",
    "    [1,1,1,1,0,0],\n",
    "    [1,0,0,0,1,0],\n",
    "    [1,1,1,1,0,0],\n",
    "    [1,0,0,0,1,0],\n",
    "    [1,1,1,1,0,0]\n",
    "], dtype=float)\n",
    "\n",
    "C = np.array([\n",
    "    [0,1,1,1,1,0],\n",
    "    [1,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0],\n",
    "    [1,0,0,0,0,1],\n",
    "    [0,1,1,1,1,0]\n",
    "], dtype=float)\n",
    "\n",
    "# helper to show image\n",
    "def show_letter(img, title=''):\n",
    "    plt.imshow(img, cmap='gray', interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.subplot(1,3,1)\n",
    "show_letter(A, 'A (5x6)')\n",
    "plt.subplot(1,3,2)\n",
    "show_letter(B, 'B (5x6)')\n",
    "plt.subplot(1,3,3)\n",
    "show_letter(C, 'C (5x6)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1f3816",
   "metadata": {},
   "source": [
    "### Create dataset\n",
    "We'll flatten each 5x6 into length-30 vectors and generate augmented samples by randomly flipping a small fraction of pixels. Labels will be one-hot encoded: A->[1,0,0], B->[0,1,0], C->[0,0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36efa192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(img):\n",
    "    return img.flatten()\n",
    "\n",
    "def augment(base_img, n_samples=100, flip_prob=0.05):\n",
    "    base = flatten(base_img)\n",
    "    samples = []\n",
    "    for _ in range(n_samples):\n",
    "        s = base.copy()\n",
    "        # flip each pixel with probability flip_prob to create noisy variants\n",
    "        flips = np.random.rand(s.size) < flip_prob\n",
    "        s[flips] = 1 - s[flips]\n",
    "        samples.append(s)\n",
    "    return np.array(samples)\n",
    "\n",
    "# create dataset\n",
    "n_per_class = 120\n",
    "XA = augment(A, n_per_class, flip_prob=0.05)\n",
    "XB = augment(B, n_per_class, flip_prob=0.05)\n",
    "XC = augment(C, n_per_class, flip_prob=0.05)\n",
    "\n",
    "X = np.vstack([XA, XB, XC])  # shape (360, 30)\n",
    "y = np.vstack([np.tile([1,0,0], (n_per_class,1)),\n",
    "               np.tile([0,1,0], (n_per_class,1)),\n",
    "               np.tile([0,0,1], (n_per_class,1))])\n",
    "\n",
    "# Shuffle dataset\n",
    "idx = np.arange(X.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "print('Dataset shape:', X.shape, 'Labels shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c296ee",
   "metadata": {},
   "source": [
    "## 2. Neural network implementation\n",
    "Network architecture:\n",
    "- Input layer: 30 neurons (pixels)\n",
    "- Hidden layer: 12 neurons (sigmoid)\n",
    "- Output layer: 3 neurons (sigmoid)\n",
    "\n",
    "Loss: Mean Squared Error (MSE)\n",
    "\n",
    "Note: The instructions requested sigmoid activation — we'll use sigmoid for both hidden and output layers and train using MSE with gradient descent (batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    # a is sigmoid(z)\n",
    "    return a * (1 - a)\n",
    "\n",
    "# Initialize weights\n",
    "def init_weights(n_in, n_hidden, n_out):\n",
    "    # small random values\n",
    "    W1 = np.random.randn(n_hidden, n_in) * 0.1\n",
    "    b1 = np.zeros((n_hidden, 1))\n",
    "    W2 = np.random.randn(n_out, n_hidden) * 0.1\n",
    "    b2 = np.zeros((n_out, 1))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Forward pass\n",
    "def forward(X_batch, W1, b1, W2, b2):\n",
    "    # X_batch: shape (n_features, batch_size)\n",
    "    Z1 = W1.dot(X_batch) + b1  # (n_hidden, batch)\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = W2.dot(A1) + b2      # (n_out, batch)\n",
    "    A2 = sigmoid(Z2)\n",
    "    cache = (X_batch, Z1, A1, Z2, A2)\n",
    "    return A2, cache\n",
    "\n",
    "# Compute MSE loss and accuracy\n",
    "def compute_loss(A2, Y):\n",
    "    # A2, Y shape: (n_out, batch)\n",
    "    m = Y.shape[1]\n",
    "    loss = np.sum((A2 - Y)**2) / (2*m)\n",
    "    return loss\n",
    "\n",
    "def compute_accuracy(A2, Y):\n",
    "    pred = np.argmax(A2, axis=0)\n",
    "    true = np.argmax(Y, axis=0)\n",
    "    return np.mean(pred == true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f3cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation (for MSE with sigmoid outputs)\n",
    "def backprop(cache, W2, A2, Y):\n",
    "    X_batch, Z1, A1, Z2, A2 = cache\n",
    "    m = X_batch.shape[1]\n",
    "    \n",
    "    # dLoss/dA2 = (A2 - Y) / m  (from MSE derivative)\n",
    "    dA2 = (A2 - Y) / m  # shape (n_out, m)\n",
    "    \n",
    "    # output layer gradients\n",
    "    dZ2 = dA2 * sigmoid_derivative(A2)  # (n_out, m)\n",
    "    dW2 = dZ2.dot(A1.T)                 # (n_out, n_hidden)\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)  # (n_out,1)\n",
    "    \n",
    "    # hidden layer\n",
    "    dA1 = W2.T.dot(dZ2)                 # (n_hidden, m)\n",
    "    dZ1 = dA1 * sigmoid_derivative(A1)  # (n_hidden, m)\n",
    "    dW1 = dZ1.dot(X_batch.T)            # (n_hidden, n_in)\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True)  # (n_hidden,1)\n",
    "    \n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e33c90",
   "metadata": {},
   "source": [
    "## 3. Training the network\n",
    "We'll use batch gradient descent for simplicity. Hyperparameters are chosen to ensure stable training for this small problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data shapes transposed (features x samples)\n",
    "X_T = X.T  # shape (30, N)\n",
    "Y_T = y.T  # shape (3, N)\n",
    "\n",
    "n_in = X_T.shape[0]\n",
    "n_hidden = 12\n",
    "n_out = Y_T.shape[0]\n",
    "\n",
    "W1, b1, W2, b2 = init_weights(n_in, n_hidden, n_out)\n",
    "\n",
    "# Training hyperparameters\n",
    "epochs = 800\n",
    "learning_rate = 0.8\n",
    "\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward\n",
    "    A2, cache = forward(X_T, W1, b1, W2, b2)\n",
    "    loss = compute_loss(A2, Y_T)\n",
    "    acc = compute_accuracy(A2, Y_T)\n",
    "    \n",
    "    # Backprop\n",
    "    dW1, db1, dW2, db2 = backprop(cache, W2, A2, Y_T)\n",
    "    \n",
    "    # Update weights (gradient descent)\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    loss_history.append(loss)\n",
    "    acc_history.append(acc)\n",
    "    \n",
    "    if (epoch+1) % 100 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.6f} - Acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdfb45a",
   "metadata": {},
   "source": [
    "## 4. Training results: Loss & Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a1e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(loss_history)\n",
    "plt.title('Training Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(acc_history)\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3a3635",
   "metadata": {},
   "source": [
    "## 5. Evaluate on original clean patterns and show predictions\n",
    "We'll run the trained network on the three clean templates (A, B, C) and display predicted class probabilities and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cd9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare single examples\n",
    "templates = np.vstack([flatten(A), flatten(B), flatten(C)])  # shape (3,30)\n",
    "templates_T = templates.T  # (30,3)\n",
    "\n",
    "A2_templates, _ = forward(templates_T, W1, b1, W2, b2)\n",
    "preds = np.argmax(A2_templates, axis=0)\n",
    "probs = A2_templates.T  # shape (3,3)\n",
    "\n",
    "for i, letter in enumerate(['A','B','C']):\n",
    "    print(f\"Letter: {letter}\")\n",
    "    print(\"Predicted class:\", ['A','B','C'][preds[i]])\n",
    "    print(\"Output probabilities:\", np.round(probs[i], 4))\n",
    "    plt.figure(figsize=(2,2))\n",
    "    show_letter(templates[i].reshape(5,6), f\"{letter} -> Pred: {['A','B','C'][preds[i]]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbce1f0",
   "metadata": {},
   "source": [
    "## 6. Test on a few noisy examples\n",
    "Let's create some noisy variants for each letter and see how the network performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d370152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(x):\n",
    "    # x shape (30,)\n",
    "    a2, _ = forward(x.reshape(-1,1), W1, b1, W2, b2)\n",
    "    return np.argmax(a2, axis=0)[0], a2.ravel()\n",
    "\n",
    "# generate a few noisy samples and predict\n",
    "for base,label in [(A,'A'), (B,'B'), (C,'C')]:\n",
    "    print('---')\n",
    "    for i in range(4):\n",
    "        s = flatten(base).copy()\n",
    "        flips = np.random.rand(s.size) < 0.08  # more noise\n",
    "        s[flips] = 1 - s[flips]\n",
    "        pred, prob = predict_single(s)\n",
    "        print(f'Base {label} noisy sample {i+1} -> Pred: {[\"A\",\"B\",\"C\"][pred]}, probs: {np.round(prob,3)}')\n",
    "        plt.figure(figsize=(2,2))\n",
    "        show_letter(s.reshape(5,6), f'{label} noisy -> Pred {[\"A\",\"B\",\"C\"][pred]}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d9dbf",
   "metadata": {},
   "source": [
    "## 7. Notes & Observations\n",
    "- Network: 30 -> 12 -> 3 with sigmoid activations.\n",
    "- Loss: Mean Squared Error (MSE) used to match the requested sigmoid outputs.\n",
    "- The dataset was synthetically augmented to provide enough training samples.\n",
    "\n",
    "You can further improve performance by using softmax + cross-entropy, adding more hidden units, or training with mini-batches and learning rate schedules.\n",
    "\n",
    "---\n",
    "\n",
    "**Files generated (if running this notebook locally):**\n",
    "- The notebook file itself `Neural_Network_Letter_Recognition.ipynb`.\n",
    "\n",
    "If you want, I can also create a `.py` script version or save this notebook to `/mnt/data` as a downloadable file. Would you like me to save it now?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
